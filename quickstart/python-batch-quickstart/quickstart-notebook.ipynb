{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from pprint import pprint\n",
    "import datetime, io, os, sys, time\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batch_auth\n",
    "import azure.batch.models as batchmodels\n",
    "from azure.batch.models import BatchErrorException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "\n",
    "    blob_name = os.path.basename(file_path)\n",
    "\n",
    "    print('Uploading file {} to container [{}]...'.format(file_path,\n",
    "                                                          container_name))\n",
    "\n",
    "    block_blob_client.create_blob_from_path(container_name,\n",
    "                                            blob_name,\n",
    "                                            file_path)\n",
    "\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2))\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(container_name,\n",
    "                                              blob_name,\n",
    "                                              sas_token=sas_token)\n",
    "\n",
    "    return batchmodels.ResourceFile(http_url=sas_url, file_path=blob_name)\n",
    "\n",
    "def get_container_sas_token(block_blob_client,\n",
    "                            container_name, blob_permissions):\n",
    "    # Obtain the SAS token for the container, setting the expiry time and\n",
    "    # permissions. In this case, no start time is specified, so the shared\n",
    "    # access signature becomes valid immediately.\n",
    "    container_sas_token = \\\n",
    "        block_blob_client.generate_container_shared_access_signature(\n",
    "            container_name,\n",
    "            permission=blob_permissions,\n",
    "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2))\n",
    "\n",
    "    return container_sas_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Files to Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file /home/quick/code/azure-batch-demo/quickstart/python-batch-quickstart/taskdata0.txt to container [input-python]...\n",
      "Uploading file /home/quick/code/azure-batch-demo/quickstart/python-batch-quickstart/taskdata1.txt to container [input-python]...\n",
      "Uploading file /home/quick/code/azure-batch-demo/quickstart/python-batch-quickstart/taskdata2.txt to container [input-python]...\n"
     ]
    }
   ],
   "source": [
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name=config._STORAGE_ACCOUNT_NAME,\n",
    "    account_key=config._STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "input_container_name = 'input-python'\n",
    "blob_client.create_container(input_container_name, fail_on_exist=False)\n",
    "\n",
    "       \n",
    "\n",
    "# The collection of data files that are to be processed by the tasks.\n",
    "input_file_paths =  [os.path.join(sys.path[0], 'taskdata0.txt'),\n",
    "                     os.path.join(sys.path[0], 'taskdata1.txt'),\n",
    "                     os.path.join(sys.path[0], 'taskdata2.txt')]\n",
    "\n",
    "# Upload the data files. \n",
    "input_files = [\n",
    "    upload_file_to_container(blob_client, input_container_name, file_path)\n",
    "    for file_path in input_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File taskdata0.txt | SAS: https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata0.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=bdj6Kjjuc42bGDPEhy/0naS9ng7adJhXio7qpftAVjI%3D\n",
      "\n",
      "File taskdata1.txt | SAS: https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata1.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=U/ckAZ2GYyFrpPgUeL1EfYtofmd78oC6rxrz4qZ2iaY%3D\n",
      "\n",
      "File taskdata2.txt | SAS: https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata2.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=8HA5mNMeKT2IdEzc59LkQQDb7EKAIdXBp/IwyNeNCpQ%3D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list of Batch ResourceFiles and their SAS urls for Azure Storage\n",
    "for file in input_files:\n",
    "    print(f\"File {file.file_path} | SAS: {file.http_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Service client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch\n",
    "# service in addition to Storage\n",
    "credentials = batch_auth.SharedKeyCredentials(config._BATCH_ACCOUNT_NAME,\n",
    "                                              config._BATCH_ACCOUNT_KEY)\n",
    "\n",
    "batch_client = batch.BatchServiceClient(\n",
    "    credentials,\n",
    "    batch_url=config._BATCH_ACCOUNT_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [PythonQuickstartPool]...\n"
     ]
    }
   ],
   "source": [
    "def create_pool(batch_service_client, pool_id):\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Create a new pool of Linux compute nodes using an Azure Virtual Machines\n",
    "    # Marketplace image. For more information about creating pools of Linux\n",
    "    # nodes, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=batchmodels.ImageReference(\n",
    "                    publisher=\"Canonical\",\n",
    "                    offer=\"UbuntuServer\",\n",
    "                    sku=\"18.04-LTS\",\n",
    "                    version=\"latest\"\n",
    "                ),\n",
    "        node_agent_sku_id=\"batch.node.ubuntu 18.04\"),\n",
    "        vm_size=config._POOL_VM_SIZE,\n",
    "        target_dedicated_nodes=config._POOL_NODE_COUNT\n",
    "    )\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except BatchErrorException:\n",
    "        print(\"Pool already exists. Reusing current pool\")\n",
    "\n",
    "# Create the pool that will contain the compute nodes that will execute the tasks.\n",
    "create_pool(batch_client, config._POOL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allocation_state': 'resizing',\n",
      " 'allocation_state_transition_time': '2019-03-29T16:59:28.755735Z',\n",
      " 'creation_time': '2019-03-29T16:59:28.755735Z',\n",
      " 'current_dedicated_nodes': 0,\n",
      " 'current_low_priority_nodes': 0,\n",
      " 'e_tag': '0x8D6B467E810E8EE',\n",
      " 'enable_auto_scale': False,\n",
      " 'enable_inter_node_communication': False,\n",
      " 'id': 'PythonQuickstartPool',\n",
      " 'last_modified': '2019-03-29T16:59:28.755735Z',\n",
      " 'max_tasks_per_node': 1,\n",
      " 'resize_timeout': 'PT15M',\n",
      " 'state': 'active',\n",
      " 'state_transition_time': '2019-03-29T16:59:28.755735Z',\n",
      " 'target_dedicated_nodes': 2,\n",
      " 'target_low_priority_nodes': 0,\n",
      " 'task_scheduling_policy': {'node_fill_type': 'spread'},\n",
      " 'url': 'https://nunos.westeurope.batch.azure.com/pools/PythonQuickstartPool',\n",
      " 'virtual_machine_configuration': {'image_reference': {'offer': 'UbuntuServer',\n",
      "                                                       'publisher': 'Canonical',\n",
      "                                                       'sku': '18.04-LTS',\n",
      "                                                       'version': 'latest'},\n",
      "                                   'node_agent_sku_id': 'batch.node.ubuntu '\n",
      "                                                        '18.04'},\n",
      " 'vm_size': 'standard_a1_v2'}\n"
     ]
    }
   ],
   "source": [
    "pool = batch_client.pool.get(config._POOL_ID)\n",
    "pprint(pool.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [PythonQuickstartJob]...\n"
     ]
    }
   ],
   "source": [
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "\n",
    "    print('Creating job [{}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(\n",
    "        id=job_id,\n",
    "        pool_info=batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    try:\n",
    "        batch_service_client.job.add(job)\n",
    "    except BatchErrorException:\n",
    "        print(\"Job already exists\")\n",
    "    \n",
    "\n",
    "# Create the job that will run the tasks.\n",
    "create_job(batch_client, config._JOB_ID, config._POOL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'constraints': {'max_task_retry_count': 0,\n",
      "                 'max_wall_clock_time': 'P10675199DT2H48M5.477581S'},\n",
      " 'creation_time': '2019-03-29T16:59:47.944139Z',\n",
      " 'e_tag': '0x8D6B467F383BB4F',\n",
      " 'execution_info': {'pool_id': 'PythonQuickstartPool',\n",
      "                    'start_time': '2019-03-29T16:59:47.963169Z'},\n",
      " 'id': 'PythonQuickstartJob',\n",
      " 'last_modified': '2019-03-29T16:59:47.963169Z',\n",
      " 'on_all_tasks_complete': 'noaction',\n",
      " 'on_task_failure': 'noaction',\n",
      " 'pool_info': {'pool_id': 'PythonQuickstartPool'},\n",
      " 'priority': 0,\n",
      " 'state': 'active',\n",
      " 'state_transition_time': '2019-03-29T16:59:47.963169Z',\n",
      " 'url': 'https://nunos.westeurope.batch.azure.com/jobs/PythonQuickstartJob',\n",
      " 'uses_task_dependencies': False}\n"
     ]
    }
   ],
   "source": [
    "job = batch_client.job.get(config._JOB_ID)\n",
    "pprint(job.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 3 tasks to job [PythonQuickstartJob]...\n"
     ]
    }
   ],
   "source": [
    "def add_tasks(batch_service_client, job_id, input_files):\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    tasks = list()\n",
    "\n",
    "    for idx, input_file in enumerate(input_files): \n",
    "\n",
    "        command = \"/bin/bash -c \\\"cat {}\\\"\".format(input_file.file_path)\n",
    "        tasks.append(batch.models.TaskAddParameter(\n",
    "                id='Task{}'.format(idx),\n",
    "                command_line=command,\n",
    "                resource_files=[input_file]\n",
    "                )\n",
    "        )\n",
    "    \n",
    "    batch_service_client.task.add_collection(job_id, tasks)\n",
    "    return tasks\n",
    "\n",
    "# Add the tasks to the job. \n",
    "tasks = add_tasks(batch_client, config._JOB_ID, input_files)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'command_line': '/bin/bash -c \"cat taskdata0.txt\"',\n",
      " 'id': 'Task0',\n",
      " 'resource_files': [{'file_path': 'taskdata0.txt',\n",
      "                     'http_url': 'https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata0.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=bdj6Kjjuc42bGDPEhy/0naS9ng7adJhXio7qpftAVjI%3D'}]}\n",
      "\n",
      "{'command_line': '/bin/bash -c \"cat taskdata1.txt\"',\n",
      " 'id': 'Task1',\n",
      " 'resource_files': [{'file_path': 'taskdata1.txt',\n",
      "                     'http_url': 'https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata1.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=U/ckAZ2GYyFrpPgUeL1EfYtofmd78oC6rxrz4qZ2iaY%3D'}]}\n",
      "\n",
      "{'command_line': '/bin/bash -c \"cat taskdata2.txt\"',\n",
      " 'id': 'Task2',\n",
      " 'resource_files': [{'file_path': 'taskdata2.txt',\n",
      "                     'http_url': 'https://nunosbatchstorage.blob.core.windows.net/input-python/taskdata2.txt?se=2019-03-29T18%3A59%3A19Z&sp=r&sv=2018-03-28&sr=b&sig=8HA5mNMeKT2IdEzc59LkQQDb7EKAIdXBp/IwyNeNCpQ%3D'}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    pprint(task.as_dict())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for tasks to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring all tasks for 'Completed' state, timeout in 0:30:00..........................................................................................................................\n",
      "  Success! All tasks reached the 'Completed' state within the specified timeout period.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    timeout_expiration = datetime.datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n",
    "          .format(timeout), end='')\n",
    "\n",
    "    while datetime.datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = batch_service_client.task.list(job_id)\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n",
    "                       \"timeout period of \" + str(timeout))\n",
    "\n",
    "wait_for_tasks_to_complete(batch_client,\n",
    "                           config._JOB_ID,\n",
    "                           datetime.timedelta(minutes=30))\n",
    "\n",
    "print(\"  Success! All tasks reached the 'Completed' state within the \"\n",
    "          \"specified timeout period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing task output...\n",
      "Task: Task0\n",
      "Node: tvm-905457941_2-20190329t170027z\n",
      "Standard output:\n",
      "Batch processing began with mainframe computers and punch cards. Today it still plays a central role in business, engineering, science, and other pursuits that require running lots of automated tasks—processing bills and payroll, calculating portfolio risk, designing new products, rendering animated films, testing software, searching for energy, predicting the weather, and finding new cures for disease. Previously only a few had access to the computing power for these scenarios. With Azure, that power is available to you when you need it, without a massive capital investment.\n",
      "\n",
      "Task: Task1\n",
      "Node: tvm-905457941_2-20190329t170027z\n",
      "Standard output:\n",
      "Azure Virtual Machines lets you deploy a wide range of computing solutions in an agile way. Deploy a virtual machine nearly instantly, and pay by the minute. With support for Microsoft Windows, Linux, Microsoft SQL Server, Oracle, IBM, SAP, and Azure BizTalk Services, you can deploy any workload and any language on nearly any operating system.\n",
      "\n",
      "Task: Task2\n",
      "Node: tvm-905457941_1-20190329t170027z\n",
      "Standard output:\n",
      "Develop, package, and deploy powerful applications and services to the cloud with Azure Cloud Services and the click of a button. Scale from 1 to 1000 in minutes. Once your application is deployed, that’s it: from provisioning, to load-balancing, to health monitoring, Azure handles the rest. Your application is backed by an industry-leading 99.95% monthly SLA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_task_output(batch_service_client, job_id, encoding=None):\n",
    "\n",
    "    print('Printing task output...')\n",
    "\n",
    "    tasks = batch_service_client.task.list(job_id)\n",
    "\n",
    "    for task in tasks:\n",
    "\n",
    "        node_id = batch_service_client.task.get(job_id, task.id).node_info.node_id\n",
    "        print(\"Task: {}\".format(task.id))\n",
    "        print(\"Node: {}\".format(node_id))\n",
    "\n",
    "        stream = batch_service_client.file.get_from_task(job_id, task.id, config._STANDARD_OUT_FILE_NAME)\n",
    "\n",
    "        file_text = _read_stream_as_string(\n",
    "            stream,\n",
    "            encoding)\n",
    "        print(\"Standard output:\")\n",
    "        print(file_text)\n",
    "\n",
    "def _read_stream_as_string(stream, encoding):\n",
    "    output = io.BytesIO()\n",
    "    try:\n",
    "        for data in stream:\n",
    "            output.write(data)\n",
    "        if encoding is None:\n",
    "            encoding = 'utf-8'\n",
    "        return output.getvalue().decode(encoding)\n",
    "    finally:\n",
    "        output.close()\n",
    "    raise RuntimeError('could not write data to stream or decode bytes')\n",
    "\n",
    "    \n",
    "# Print the stdout.txt and stderr.txt files for each task to the console\n",
    "print_task_output(batch_client, config._JOB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting container [input-python]...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up storage resources\n",
    "print('Deleting container [{}]...'.format(input_container_name))\n",
    "blob_client.delete_container(input_container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete job\n",
    "batch_client.job.delete(config._JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete pool\n",
    "batch_client.pool.delete(config._POOL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
